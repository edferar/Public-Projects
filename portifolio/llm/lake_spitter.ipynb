{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12188207",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-10-24T20:20:57.294238Z",
          "iopub.status.busy": "2025-10-24T20:20:57.293457Z",
          "iopub.status.idle": "2025-10-24T20:20:57.302473Z",
          "shell.execute_reply": "2025-10-24T20:20:57.301296Z"
        },
        "id": "12188207",
        "outputId": "a76e70c6-c279-4f19-f491-7cb51d009424",
        "papermill": {
          "duration": 0.013511,
          "end_time": "2025-10-24T20:20:57.303990",
          "exception": false,
          "start_time": "2025-10-24T20:20:57.290479",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Um cuspidor esta nascendo!\n"
          ]
        }
      ],
      "source": [
        "print(\"Um cuspidor esta nascendo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7waazcN7jlrB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7waazcN7jlrB",
        "outputId": "00c5d8eb-8823-4f9c-8548-9e14d092802c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-readers-wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nilEBfU7_3ao",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nilEBfU7_3ao",
        "outputId": "4d2219de-2879-496e-8555-24e4d8e324c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio duckdb duckdb-engine pandas unidecode --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "CSxx8LkUhmdG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSxx8LkUhmdG",
        "outputId": "cafa02c9-6c82-4d47-9a82-3f29d4500610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.3/191.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-experimental llama-index-llms-groq -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scwa3Frgg7jW",
      "metadata": {
        "id": "scwa3Frgg7jW"
      },
      "source": [
        "# Como Obter e Usar a API Key da Groq\n",
        "\n",
        "## 1. Criar Conta ou Fazer Login\n",
        "- Acesse [Groq Console](https://console.groq.com/login) e crie uma conta ou faça login se já possuir.\n",
        "\n",
        "## 2. Navegar até a Seção de API Keys\n",
        "- No painel da sua conta, vá até a página de [API Keys](https://console.groq.com/keys).\n",
        "\n",
        "## 3. Criar uma Nova API Key\n",
        "- Clique em **\"Create API Key\"**.\n",
        "- Dê um nome descritivo à chave (por exemplo: `Colab Integration`) e confirme a criação.\n",
        "\n",
        "## 4. Copiar a API Key\n",
        "- Após a criação, copie a API key exibida.\n",
        "- **Atenção:** Essa será a única vez que a chave será mostrada. Guarde-a em um local seguro.\n",
        "\n",
        "## 5. Configurar a Chave no Colab\n",
        "- **Boa prática:** Não coloque a chave diretamente no código. Em vez disso, use **Colab Secrets** ou variáveis de ambiente.\n",
        "\n",
        "Exemplo usando variável de ambiente:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "# Defina a variável de ambiente\n",
        "os.environ[\"GROQ_API_KEY\"] = \"sua-api-key-aqui\"\n",
        "\n",
        "#ou ustilizando secret\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "1ID-l-qMipNf"
      },
      "id": "1ID-l-qMipNf",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "class SpitterDucklakeEngines():\n",
        "  @classmethod\n",
        "  def get_session_duckdb(self, mode, ducklake_db, path, layer=False):\n",
        "    engine = create_engine(\"duckdb:///:memory:\")\n",
        "\n",
        "    with engine.connect() as con:\n",
        "      con.exec_driver_sql(\"INSTALL 'ducklake';\")\n",
        "      con.exec_driver_sql(\"LOAD 'ducklake';\")\n",
        "      #con.exec_driver_sql(f\"DETACH DATABASE IF EXISTS {ducklake_db};\")\n",
        "      con.exec_driver_sql(f\"\"\"ATTACH 'ducklake:metadata.ducklake' AS {ducklake_db} (DATA_PATH '{path}');\"\"\")\n",
        "\n",
        "      result = con.exec_driver_sql(f\"\"\"SHOW DATABASES;\"\"\").fetchall()\n",
        "      print(\"Databases anexados:\", [i[0] for i in result])\n",
        "      if layer:\n",
        "        con.exec_driver_sql(f\"CREATE SCHEMA IF NOT EXISTS {ducklake_db}.{layer}\")\n",
        "        con.exec_driver_sql(f\"USE {ducklake_db}.{layer};\")\n",
        "      else:\n",
        "        con.exec_driver_sql(f\"USE {ducklake_db};\")\n",
        "\n",
        "      result = con.exec_driver_sql(f\"\"\"SHOW TABLES;\"\"\").fetchall()\n",
        "      tables = [i[0] for i in result]\n",
        "      print(\"Tabelas identificadas:\", tables)\n",
        "      con.commit()\n",
        "\n",
        "    if mode == 'engine':\n",
        "      return engine, tables\n",
        "    elif mode == 'connection':\n",
        "      return engine.connect()\n",
        "    else:\n",
        "      raise ValueError(\"mode deve ser 'engine' ou 'connection'.\")\n",
        "\n",
        "  @classmethod\n",
        "  def close_connection(self, engine):\n",
        "    if engine:\n",
        "      engine.close()\n",
        "  def dispose_connection(self, engine):\n",
        "    if engine:\n",
        "      engine.dispose()\n",
        "\n",
        "\n",
        "\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import SQLDatabase\n",
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "class SpitterDucklakeAnalystOperator():\n",
        "  def __init__(self,groq_model='llama-3.3-70b-versatile'):\n",
        "    self.llm = Groq(model=groq_model, api_key=self.get_groq_environment())\n",
        "    self.engine_op = SpitterDucklakeEngines()\n",
        "    self.engine = None\n",
        "    self.query_engine = None\n",
        "\n",
        "\n",
        "  def get_groq_environment(self):\n",
        "    if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "        raise ValueError(\"A chave GROQ_API_KEY não foi fornecida nem encontrada no ambiente.\")\n",
        "    return os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "  def get_query_engine(self, ducklake_db, path, layer, tables=[]):\n",
        "      self.engine, database_tables = self.engine_op.get_session_duckdb('engine', ducklake_db, path, layer)\n",
        "      tables = tables if tables else database_tables\n",
        "      sql_database = SQLDatabase(self.engine, include_tables=tables)\n",
        "\n",
        "      return NLSQLTableQueryEngine(sql_database, embed_model=self.llm)\n",
        "\n",
        "  def close(self):\n",
        "    self.engine_op.dispose_connection(self.engine)\n"
      ],
      "metadata": {
        "id": "xzZ6xbNKiqeg"
      },
      "id": "xzZ6xbNKiqeg",
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tables = ['tb_bronze_escolas_gramame_geisel']\n",
        "ducklake = \"spitter_ducklake\"\n",
        "path = \"/tmp/dklq_spitter_ducklake\"\n",
        "layer = \"bronze\"\n",
        "op = SpitterDucklakeAnalystOperator()\n",
        "query_engine = op.get_query_engine(ducklake, path, layer, tables)\n",
        "query_engine.query(\"Qual bairro possui mais escolas?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajJyeqf2vn6w",
        "outputId": "c1ba293f-acac-49af-a824-6c706497d35b"
      },
      "id": "ajJyeqf2vn6w",
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Databases anexados: ['memory', 'spitter_ducklake']\n",
            "Tabelas identificadas: ['tb_bronze_escolas_gramame_geisel']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response='O bairro com mais escolas é Gramame, com 8 escolas. Em seguida, vem o bairro Ernesto Geisel, com 6 escolas.', source_nodes=[NodeWithScore(node=TextNode(id_='43c31afe-8263-4cb9-bcc1-29b2a3060531', embedding=None, metadata={'sql_query': 'SELECT tb_bronze_escolas_gramame_geisel.bairro, COUNT(tb_bronze_escolas_gramame_geisel.escola) AS num_escolas FROM tb_bronze_escolas_gramame_geisel GROUP BY tb_bronze_escolas_gramame_geisel.bairro ORDER BY num_escolas DESC', 'result': [('Gramame', 8), ('Ernesto Geisel', 6)], 'col_keys': ['bairro', 'num_escolas']}, excluded_embed_metadata_keys=['sql_query', 'result', 'col_keys'], excluded_llm_metadata_keys=['sql_query', 'result', 'col_keys'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"[('Gramame', 8), ('Ernesto Geisel', 6)]\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)], metadata={'43c31afe-8263-4cb9-bcc1-29b2a3060531': {'sql_query': 'SELECT tb_bronze_escolas_gramame_geisel.bairro, COUNT(tb_bronze_escolas_gramame_geisel.escola) AS num_escolas FROM tb_bronze_escolas_gramame_geisel GROUP BY tb_bronze_escolas_gramame_geisel.bairro ORDER BY num_escolas DESC', 'result': [('Gramame', 8), ('Ernesto Geisel', 6)], 'col_keys': ['bairro', 'num_escolas']}, 'sql_query': 'SELECT tb_bronze_escolas_gramame_geisel.bairro, COUNT(tb_bronze_escolas_gramame_geisel.escola) AS num_escolas FROM tb_bronze_escolas_gramame_geisel GROUP BY tb_bronze_escolas_gramame_geisel.bairro ORDER BY num_escolas DESC', 'result': [('Gramame', 8), ('Ernesto Geisel', 6)], 'col_keys': ['bairro', 'num_escolas']})"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.close()"
      ],
      "metadata": {
        "id": "DPv_xCFSC8r4"
      },
      "id": "DPv_xCFSC8r4",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H02_f2n923JV"
      },
      "id": "H02_f2n923JV"
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "x68_uXWeiepn",
      "metadata": {
        "id": "x68_uXWeiepn"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "class SpitterDucklakeEngenieertOperator():\n",
        "  def __init__(self,groq_model='llama-3.3-70b-versatile'):\n",
        "\n",
        "    self.query_engine = None\n",
        "    self.engine_op = SpitterDucklakeEngines()\n",
        "    self.engine = None\n",
        "    Settings.llm = Groq(model='llama-3.3-70b-versatile', api_key=self.get_groq_environment())\n",
        "\n",
        "\n",
        "  def get_groq_environment(self):\n",
        "    if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "        raise ValueError(\"A chave GROQ_API_KEY não foi fornecida nem encontrada no ambiente.\")\n",
        "    return os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "  def get_table_statistcs(self, table, layer, ducklake_db, path):\n",
        "      with self.engine.connect() as con:\n",
        "        desc = con.exec_driver_sql(f\"\"\"DESCRIBE FROM {table}\"\"\").fetchall()\n",
        "        cols = \",\".join([f'\"{row[0]}\"' for row in desc])\n",
        "        ref_query = f\"SELECT {cols} FROM {table}\"\n",
        "\n",
        "        desc_str = \"\\n\".join([f\"{col}: {dtype}\" for col, dtype, *_ in desc])\n",
        "        sample_df = con.exec_driver_sql(ref_query).fetchall()\n",
        "        sample_df = pd.DataFrame(sample_df, columns=[row[0] for row in desc])\n",
        "        sample_df = sample_df.apply(lambda col: col.astype(str).str.slice(0, 50) if col.dtype == \"object\" else col)\n",
        "        sample_text = sample_df.to_string(index=False)\n",
        "\n",
        "      return desc_str, sample_text, ref_query\n",
        "\n",
        "\n",
        "  def generate_sql(self, query, table, layer, ducklake_db, path, last_query=False):\n",
        "      llm = Settings.llm\n",
        "      self.engine, database_tables = self.engine_op.get_session_duckdb('engine', ducklake_db, path, layer)\n",
        "      schema_table, sample_table,ref_query = self.get_table_statistcs(table, layer, ducklake_db, path)\n",
        "      ref_query = last_query if last_query else ref_query\n",
        "\n",
        "      \"\"\"\"\n",
        "          Gere a consulta com base nas colunas da tabela.\\n\n",
        "          Modifique apenas os campos solicitados; todas as demais partes da query devem permanecer iguais.\\n\n",
        "          Caso solicitado, voce devera remover campos da query e desfazer alteracões.\\n\n",
        "          A query final deve ser uma expressão SQL válida e executável em SQL ANSI.\\n\n",
        "          Retorne apenas a query, sem explicações, comentários ou texto adicional, sem aspas.\\n\n",
        "\n",
        "      \"\"\"\n",
        "      prompt = (\n",
        "          \"Você está trabalhando com uma tabela chamada {table} SQL presente no duckdb.\\n\"\n",
        "          \"Descrição:\\n\"\n",
        "          \"{schema_table}\\n\\n\"\n",
        "          \"Algumas linhas da tabela\\n\"\n",
        "          \"{sample_table}\\n\\n\"\n",
        "\n",
        "          \"Query atual:\\n\"\n",
        "          \"{last_query}\\n\\n\"\n",
        "\n",
        "          \"Nova instrução:\\n\"\n",
        "          \"{query}\\n\\n\"\n",
        "\n",
        "          \"Gere uma **nova query SQL** baseada na query atual, alterando apenas o necessário.\\n\"\n",
        "          \"A query final deve ser executável em SQL ANSI.\\n\"\n",
        "          \"Retorne apenas a query, sem aspas ou complemento.\\n\")\n",
        "\n",
        "\n",
        "      sql_prompt = PromptTemplate(prompt).partial_format(\n",
        "          schema_table=schema_table,\n",
        "          table=table,\n",
        "          sample_table=sample_table,\n",
        "          query=query,\n",
        "          last_query = last_query )\n",
        "\n",
        "      sql_query = llm.predict(sql_prompt)\n",
        "\n",
        "      return sql_query\n",
        "\n",
        "      def close(self):\n",
        "        self.engine_op.dispose_connection(self.engine)\n",
        "\n",
        "table = \"tb_bronze_escolas_gramame_geisel\"\n",
        "layer=\"bronze\"\n",
        "query_gen = False\n",
        "op = SpitterDucklakeEngenieertOperator()\n",
        "\n",
        "ducklake = \"spitter_ducklake\"\n",
        "path = \"/tmp/dklq_spitter_ducklake\"\n",
        "layer = \"bronze\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "ity4USVqfivO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ity4USVqfivO",
        "outputId": "0ea47b6b-9e1b-481f-a1a9-ae13e1e4078c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Databases anexados: ['memory', 'spitter_ducklake']\n",
            "Tabelas identificadas: ['tb_bronze_escolas_gramame_geisel']\n",
            "\n",
            "--- QUERY SQL GERADA ---\n",
            "SELECT escola FROM tb_bronze_escolas_gramame_geisel\n"
          ]
        }
      ],
      "source": [
        "query = \"Inclua a coluna escola na query\"\n",
        "query_gen = op.generate_sql(query, table, layer, ducklake, path, last_query=query_gen)\n",
        "print(\"\\n--- QUERY SQL GERADA ---\")\n",
        "print(query_gen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.close()"
      ],
      "metadata": {
        "id": "sksCMEgfDlE8"
      },
      "id": "sksCMEgfDlE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xvnA39l5fh8A",
      "metadata": {
        "id": "xvnA39l5fh8A"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "mp1gCpwvlQ5m",
      "metadata": {
        "id": "mp1gCpwvlQ5m"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "YUJttku7PY-t",
      "metadata": {
        "id": "YUJttku7PY-t"
      },
      "outputs": [],
      "source": [
        "from unidecode import unidecode\n",
        "import traceback\n",
        "import os\n",
        "\n",
        "import re\n",
        "\n",
        "class SpitterDucklakeOperator():\n",
        "    def __init__(self, con, ducklake_db, path=None):\n",
        "\n",
        "      self.ducklake_db = ducklake_db\n",
        "      self.con = con\n",
        "\n",
        "    def table_exist(self, table):\n",
        "      try:\n",
        "        self.con.exec_driver_sql(f\"SELECT * FROM {table} LIMIT 0\")\n",
        "        return True\n",
        "      except Exception as e:\n",
        "        return False\n",
        "\n",
        "    def normalize_name(self, col):\n",
        "      col = unidecode(col).lower().strip()\n",
        "      col = re.sub(r'[^a-z0-9_]', '_', col)\n",
        "      col = re.sub(r'_+', '_', col)\n",
        "      return col.strip('_').encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
        "\n",
        "    def create_or_replace_table(self, source_table_name, output_table_name, layer, primary_key=False):\n",
        "      try:\n",
        "        #self.con.exec_driver_sql(f\"CREATE SCHEMA IF NOT EXISTS {self.ducklake_db}.{layer}\")\n",
        "        instruction_sql = ','.join([\"{0} {1} {2}\".format(\n",
        "                                                          self.normalize_name(row[0]),\n",
        "                                                          row[1],\n",
        "                                                          \"\" if row[0] != primary_key else 'PRIMARY KEY'\n",
        "                                                        ) for row in self.con.exec_driver_sql(f\"DESCRIBE FROM {source_table_name}\").fetchall()\n",
        "                                  ]\n",
        "                                    )\n",
        "\n",
        "        self.con.exec_driver_sql(f\"\"\"CREATE OR REPLACE TABLE {self.ducklake_db}.{layer}.{output_table_name} ({instruction_sql})\"\"\")\n",
        "        self.con.commit()\n",
        "        return True, \"success\"\n",
        "      except Exception as e:\n",
        "        return False, f\"{e} -> {traceback.format_exc()}\"\n",
        "\n",
        "    def insert_table(self, source_table_name, output_table_name, layer,op_mode, primary_key=False):\n",
        "      try:\n",
        "        if not self.table_exist(f\"{self.ducklake_db}.{layer}.{output_table_name}\") or op_mode == \"overwrite\":\n",
        "            self.create_or_replace_table(source_table_name, output_table_name, layer, primary_key)\n",
        "            self.con.commit()\n",
        "        if primary_key:\n",
        "          self.con.exec_driver_sql(f\"INSERT OR REPLACE INTO {self.ducklake_db}.{layer}.{output_table_name} SELECT * FROM {source_table_name}\")\n",
        "          self.con.commit()\n",
        "        else:\n",
        "          self.con.exec_driver_sql(f\"INSERT INTO {self.ducklake_db}.{layer}.{output_table_name} SELECT * FROM {source_table_name}\")\n",
        "          self.con.commit()\n",
        "        return True, \"success\"\n",
        "      except Exception as e:\n",
        "        return False, f\"{e} -> {traceback.format_exc()}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "-8fKAy_MFPm9",
      "metadata": {
        "id": "-8fKAy_MFPm9"
      },
      "outputs": [],
      "source": [
        "#csv operations\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import traceback\n",
        "\n",
        "class SpitterOperator():\n",
        "    def __init__(self, file, ducklake_db=\"spitter_ducklake\", path=None):\n",
        "        self.df = pd.DataFrame()\n",
        "        self.primary_key = \"\"\n",
        "        self.cols = []\n",
        "        self.ducklake_db = ducklake_db\n",
        "        self.path_prefix = os.environ.get(\"TMP\").replace(\"\\\\\",\"/\") if os.name == \"nt\" else \"/tmp\"\n",
        "        self.path = path if path else f\"{self.path_prefix}/dklq_{self.ducklake_db}\"\n",
        "        print(self.path)\n",
        "        self.con =  SpitterDucklakeEngines.get_session_duckdb('connection', self.ducklake_db, self.path)\n",
        "        self.dklq = SpitterDucklakeOperator(self.con, self.ducklake_db)\n",
        "        self.tables = {}\n",
        "        self.op_mode = \"\"\n",
        "        if file is not None:\n",
        "            self.load_bronze(file)\n",
        "        else:\n",
        "            return \"Envie um file CSV.\"\n",
        "\n",
        "    def preview_bronze(self, layer='bronze'):\n",
        "        norm_cols = \",\".join([f'\"{col}\" AS {self.dklq.normalize_name(col)}' for col in self.cols])\n",
        "        preview_df = self.con.exec_driver_sql(f\"\"\"\n",
        "          SELECT {norm_cols} FROM tmp_{self.tables[layer]} LIMIT 10\n",
        "        \"\"\")\n",
        "        preview_df = pd.DataFrame(preview_df.fetchall(), columns=preview_df.keys())\n",
        "        return preview_df\n",
        "\n",
        "    def set_primary_key(self, column_name):\n",
        "        if column_name in self.df.columns:\n",
        "            self.primary_key = column_name\n",
        "\n",
        "    def set_table_op_mode(self, op_mode):\n",
        "        self.op_mode = op_mode\n",
        "\n",
        "    def persist_table_on_dklq(self, layer):\n",
        "        result = self.dklq.insert_table(f\"tmp_{self.tables[layer]}\", self.tables[layer], layer, self.op_mode, self.primary_key)\n",
        "        if not result[0]:\n",
        "          raise Exception(result[1])\n",
        "\n",
        "    def close_connection(self):\n",
        "        SpitterDucklakeEngines.close_connection(self.con)\n",
        "\n",
        "    def load_bronze(self,file,layer='bronze'):\n",
        "\n",
        "        try:\n",
        "\n",
        "          table= self.dklq.normalize_name(file.name.replace(\"\\\\\", \"/\").split(\"/\")[-1].split(\".\")[0])\n",
        "          self.tables[layer]= f\"tb_{layer}_{table}\"\n",
        "          self.con.exec_driver_sql(f\"\"\"\n",
        "              CREATE TEMPORARY TABLE tmp_{self.tables[layer]} AS\n",
        "              SELECT * FROM read_csv_auto('{file.name}', header=True)\n",
        "          \"\"\")\n",
        "          self.cols = [row[0] for row in self.con.exec_driver_sql(f\"DESCRIBE tmp_{self.tables[layer]}\").fetchall()]\n",
        "        except Exception as e:\n",
        "          raise (f\"{e} -> {traceback.format_exc()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "PfmR2fRj_6XA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PfmR2fRj_6XA",
        "outputId": "108fd6d0-1515-426c-88b8-196562512568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7e4440fb9c37fdc649.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7e4440fb9c37fdc649.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 389, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7b5f56b03dd0 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/dklq_spitter_ducklake\n",
            "Databases anexados: ['memory', 'spitter_ducklake']\n",
            "Tabelas identificadas: []\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7e4440fb9c37fdc649.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def init_file_operator(file):\n",
        "    spitter_operator = SpitterOperator(file)\n",
        "    return spitter_operator, \"bronze\"\n",
        "\n",
        "def preview_bronze(spitter_operator):\n",
        "    return spitter_operator.preview_bronze()\n",
        "\n",
        "def set_pk_column(spitter_operator, pk_column):\n",
        "    if spitter_operator is not None:\n",
        "        spitter_operator.set_primary_key(pk_column)\n",
        "    return spitter_operator\n",
        "\n",
        "#button options\n",
        "def set_pk_chk_visibility(spitter_operator, checkbox_value):\n",
        "    if checkbox_value:\n",
        "        return gr.update(interactive=True, visible=True,choices=spitter_operator.cols)\n",
        "    else:\n",
        "        return gr.update(interactive=False, visible=False)\n",
        "\n",
        "def set_op_chk_visibility(spitter_operator, layer):\n",
        "    choices = [\"append\", \"overwrite\"] if layer == \"bronze\" else [\"overwrite\",\"upsert\"]\n",
        "    return gr.update(interactive=True, visible=True, choices= choices)\n",
        "\n",
        "\n",
        "def set_table_op_mode_btn(spitter_operator, op_mode, layer):\n",
        "  spitter_operator.set_table_op_mode(op_mode)\n",
        "  return f\"Commit - table -> {spitter_operator.tables[layer]} | op_mode -> {op_mode}\", True\n",
        "\n",
        "def publish_bronze_table(spitter_operator, commit_state, layer):\n",
        "  if commit_state:\n",
        "    spitter_operator.persist_table_on_dklq(layer)\n",
        "    spitter_operator.close_connection()\n",
        "    return \"sucess\"\n",
        "  else:\n",
        "    return \"Error. Please check your settings!\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"# Arquitetura Medalhão - Data Spitter - CSV - multi Reads\") as spitter_demo:\n",
        "    gr.Markdown(\"# Arquitetura Medalhão (Bronze / Silver / Gold)\")\n",
        "    gr.Markdown(\"## Spitter Data Lakehouse Explorer\")\n",
        "    gr.Markdown(\"## Leia seus dados, configure as operacoes, e maos a obra. Apresentamos suporte a cargas incrementais e de sobrescrita\")\n",
        "    gr.Markdown(\"As tabelas criadas utilizarao o nome do arquivo como referencia\")\n",
        "    st_spitter_operator = gr.State(None)\n",
        "    st_layer = gr.State(\"\")\n",
        "\n",
        "    #BronzeLayer\n",
        "    with gr.Tab(\"Bronze Layer\"):\n",
        "        gr.Markdown(\"### Upload dos dados brutos\")\n",
        "        st_choices = gr.State([])\n",
        "        st_commit_bronze = gr.State(False)\n",
        "\n",
        "        csv_bronze = gr.File(label=\"Selecione um CSV\",file_count=\"single\", type=\"filepath\")\n",
        "        csv_bronze.upload(init_file_operator, inputs=csv_bronze, outputs=[st_spitter_operator,st_layer])\n",
        "\n",
        "\n",
        "        bronze_btn = gr.Button(\"Preview Bronze\")\n",
        "        bronze_table = gr.DataFrame(label=\"Bronze Data\")\n",
        "        bronze_btn.click(preview_bronze, inputs=st_spitter_operator, outputs=bronze_table)\n",
        "\n",
        "        gr.Markdown(\"### Limpeza dos dados brutos\")\n",
        "\n",
        "\n",
        "        op_dropdown = gr.Dropdown(label=\"Select the table operaration mode (required)\", choices=[], interactive=False, visible=False)\n",
        "        st_spitter_operator.change(set_op_chk_visibility,inputs=[st_spitter_operator, st_layer],outputs=op_dropdown)\n",
        "\n",
        "        output_text = gr.Markdown(\"\")\n",
        "        op_dropdown.change(set_table_op_mode_btn, inputs=[st_spitter_operator, op_dropdown, st_layer], outputs=[output_text, st_commit_bronze])\n",
        "\n",
        "        alerta_md_component = gr.Markdown(\"\")\n",
        "        publish_bronze_btn = gr.Button(value=f\"Commit\", visible=True)\n",
        "        publish_bronze_btn.click(publish_bronze_table, inputs=[st_spitter_operator, st_commit_bronze, st_layer], outputs=alerta_md_component)\n",
        "\n",
        "    #SilverLayer\n",
        "    with gr.Tab(\"Camada Silver\"):\n",
        "\n",
        "        pk_checkbox = gr.Checkbox(label=\"Does the table have a primary key?\", value=False)\n",
        "        pk_dropdown = gr.Dropdown(label=\"Select the primary key\", choices=[], interactive=False, visible=False)\n",
        "        pk_checkbox.change(set_pk_chk_visibility, inputs=[st_spitter_operator, pk_checkbox], outputs=pk_dropdown)\n",
        "        pk_dropdown.change(set_pk_column, inputs=[st_spitter_operator, pk_dropdown], outputs=st_spitter_operator)\n",
        "\n",
        "\n",
        "\n",
        "spitter_demo.launch(debug=True)\n",
        "#spitter_demo.queue(show_error=True)\n",
        "#spitter_demo.launch(share=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31153,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5.226134,
      "end_time": "2025-10-24T20:20:57.625124",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-10-24T20:20:52.398990",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}